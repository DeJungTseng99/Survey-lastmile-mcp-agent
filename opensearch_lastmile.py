import asyncio
import time

from pydantic import BaseModel

from mcp_agent.app import MCPApp
from mcp_agent.config import (
    GoogleSettings,
    Settings,
    LoggerSettings,
    MCPSettings,
    MCPServerSettings,
)
from mcp_agent.agents.agent import Agent
from mcp_agent.workflows.llm.augmented_llm_google import GoogleAugmentedLLM


class SearchResult(BaseModel):
    query: str
    total_hits: int
    results: list
    summary: str


settings = Settings(
    execution_engine="asyncio",
    logger=LoggerSettings(type="console", level="debug"),
    mcp=MCPSettings(
        servers={
            "opensearch": MCPServerSettings(
                transport="streamable_http",
                url="http://localhost:9900/mcp",
                headers={
                    "Accept": "application/json, text/event-stream",
                    "Content-Type": "application/json"
                },
                description="OpenSearch MCP Server for data querying"
            ),
        }
    ),
    google=GoogleSettings(
        default_model="gemini-2.0-flash",
    ),
)

# ä½¿ç”¨é…ç½®æª”æ¡ˆè€Œä¸æ˜¯ç¨‹å¼åŒ–è¨­å®šï¼Œé€™æ¨£æ‰èƒ½è®€å–secrets
app = MCPApp(name="opensearch_agent")


async def test_connection():
    """Test connection to OpenSearch MCP server and list available tools"""
    async with app.run() as agent_app:
        logger = agent_app.logger
        context = agent_app.context

        logger.info("Testing connection to OpenSearch MCP server...")
        print("\n=== æ¸¬è©¦OpenSearch MCPé€£ç·š ===")

        opensearch_agent = Agent(
            name="opensearch_tester",
            instruction="Test agent for connection verification",
            server_names=["opensearch"],
        )

        try:
            async with opensearch_agent:
                logger.info("opensearch_tester: å˜—è©¦é€£æ¥åˆ°MCP server...")
                print("âœ… æˆåŠŸå»ºç«‹èˆ‡OpenSearch MCP serverçš„é€£ç·š")
                
                # List available tools
                tools_result = await opensearch_agent.list_tools()
                logger.info("Tools discovered:", data=tools_result.model_dump())
                
                print(f"\nğŸ“‹ ç™¼ç¾ {len(tools_result.tools)} å€‹å¯ç”¨å·¥å…·:")
                for i, tool in enumerate(tools_result.tools, 1):
                    print(f"   {i}. {tool.name}")
                    if tool.description:
                        print(f"      æè¿°: {tool.description[:100]}...")
                
                # List available prompts
                try:
                    prompts_result = await opensearch_agent.list_prompts()
                    print(f"\nğŸ“ ç™¼ç¾ {len(prompts_result.prompts)} å€‹å¯ç”¨æç¤º:")
                    for i, prompt in enumerate(prompts_result.prompts, 1):
                        print(f"   {i}. {prompt.name}")
                        if prompt.description:
                            print(f"      æè¿°: {prompt.description}")
                except Exception as e:
                    print(f"âš ï¸ ç„¡æ³•åˆ—å‡ºæç¤º: {e}")
                
                return tools_result
                
        except Exception as e:
            logger.error(f"é€£ç·šå¤±æ•—: {e}")
            print(f"âŒ é€£ç·šå¤±æ•—: {e}")
            return None


async def example_usage():
    async with app.run() as agent_app:
        logger = agent_app.logger
        context = agent_app.context

        logger.info("Current config:", data=context.config.model_dump())

        opensearch_agent = Agent(
            name="opensearch_searcher",
            instruction="""You are an OpenSearch query agent with access to search capabilities. Please respond in Traditional Chinese (ç¹é«”ä¸­æ–‡).
            ä½ æ˜¯ä¸€å€‹OpenSearchæŸ¥è©¢åŠ©æ‰‹ï¼Œå…·æœ‰æœå°‹åŠŸèƒ½ã€‚è«‹ç”¨ç¹é«”ä¸­æ–‡å›æ‡‰ã€‚
            Your job is to (ä½ çš„å·¥ä½œæ˜¯):
            1. Understand user search requests and translate them into appropriate OpenSearch queries (ç†è§£ä½¿ç”¨è€…çš„æœå°‹è«‹æ±‚ä¸¦è½‰æ›ç‚ºé©ç•¶çš„OpenSearchæŸ¥è©¢)
            2. Execute the search using available tools (ä½¿ç”¨å¯ç”¨å·¥å…·åŸ·è¡Œæœå°‹)
            3. Generate proper JSON-RPC 2.0 tool calls to the OpenSearch server (ç”Ÿæˆæ­£ç¢ºçš„JSON-RPC 2.0å·¥å…·å‘¼å«)
            4. Format and summarize the search results for the user (ç‚ºä½¿ç”¨è€…æ ¼å¼åŒ–å’Œç¸½çµæœå°‹çµæœ)
            5. Ask for clarification if the search query is ambiguous (å¦‚æœæœå°‹æŸ¥è©¢ä¸æ˜ç¢ºè«‹è¦æ±‚æ¾„æ¸…)""",
            server_names=["opensearch"],
        )

        async with opensearch_agent:
            logger.info("opensearch_searcher: Connected to server, calling list_tools...")
            result = await opensearch_agent.list_tools()
            logger.info("Tools available:", data=result.model_dump())

            llm = await opensearch_agent.attach_llm(GoogleAugmentedLLM)

            # Interactive search loop
            print("\n=== OpenSearch Agent å·²å•Ÿå‹• ===")
            print("è«‹è¼¸å…¥æ‚¨çš„æœå°‹æŸ¥è©¢ï¼Œè¼¸å…¥ 'quit' é€€å‡º")
            
            while True:
                try:
                    user_query = input("\nğŸ” è«‹è¼¸å…¥æœå°‹æŸ¥è©¢: ").strip()
                    
                    if user_query.lower() in ['quit', 'exit', 'é€€å‡º']:
                        print("æ„Ÿè¬ä½¿ç”¨ OpenSearch Agent!")
                        break
                    
                    if not user_query:
                        print("è«‹è¼¸å…¥æœ‰æ•ˆçš„æœå°‹æŸ¥è©¢")
                        continue
                    
                    print(f"\nâ³ æ­£åœ¨åŸ·è¡Œæœå°‹: {user_query}")
                    
                    # Execute search query
                    result = await llm.generate_str(
                        message=f"Execute search query in OpenSearch: {user_query}",
                    )
                    logger.info(f"Search result for '{user_query}': {result}")
                    print(f"\nğŸ“Š æœå°‹çµæœ:\n{result}")
                    
                    # Generate structured summary only if we got results
                    if result and len(result.strip()) > 0:
                        try:
                            structured_result = await llm.generate_structured(
                                message="Create a structured summary of the previous search results, including the query, total hits found, and a brief summary.",
                                response_model=SearchResult,
                            )
                            print(f"\nğŸ“‹ çµæ§‹åŒ–æ‘˜è¦:")
                            print(f"   æŸ¥è©¢: {structured_result.query}")
                            print(f"   ç¸½å‘½ä¸­æ•¸: {structured_result.total_hits}")
                            print(f"   æ‘˜è¦: {structured_result.summary}")
                            logger.info(f"Structured search result: {structured_result}")
                        except Exception as e:
                            print(f"âš ï¸ çµæ§‹åŒ–æ‘˜è¦ç”Ÿæˆå¤±æ•—: {e}")
                    else:
                        print("âš ï¸ æ²’æœ‰ç²å¾—æœå°‹çµæœï¼Œè·³éçµæ§‹åŒ–æ‘˜è¦")
                    
                except KeyboardInterrupt:
                    print("\n\næ”¶åˆ°ä¸­æ–·ä¿¡è™Ÿï¼Œæ­£åœ¨é€€å‡º...")
                    break
                except Exception as e:
                    logger.error(f"åŸ·è¡Œæœå°‹æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")
                    print(f"âŒ æœå°‹å¤±æ•—: {e}")


async def demo_usage():
    """Demo mode with predefined queries"""
    async with app.run() as agent_app:
        logger = agent_app.logger
        
        opensearch_agent = Agent(
            name="opensearch_searcher", 
            instruction="""You are an OpenSearch query agent. Execute searches and return results in a clear format.""",
            server_names=["opensearch"],
        )

        async with opensearch_agent:
            llm = await opensearch_agent.attach_llm(GoogleAugmentedLLM)
            
            # Demo queries
            demo_queries = [
                "search for documents containing 'machine learning'",
                "find all entries with status='active' and timestamp from last week",
                "lookup user profiles where role='admin'"
            ]
            
            for query in demo_queries:
                print(f"\nğŸ” DemoæŸ¥è©¢: {query}")
                result = await llm.generate_str(message=f"Execute OpenSearch query: {query}")
                logger.info(f"Demo result: {result}")
                print(f"ğŸ“Š çµæœ: {result}")


if __name__ == "__main__":
    import sys
    
    start = time.time()
    
    # Check for different modes
    if len(sys.argv) > 1:
        mode = sys.argv[1]
        if mode == "test":
            print("ğŸ”§ å•Ÿå‹•é€£ç·šæ¸¬è©¦æ¨¡å¼...")
            asyncio.run(test_connection())
        elif mode == "demo":
            print("ğŸš€ å•Ÿå‹• Demo æ¨¡å¼...")
            asyncio.run(demo_usage())
        else:
            print(f"æœªçŸ¥æ¨¡å¼: {mode}")
            print("å¯ç”¨æ¨¡å¼: test, demo, æˆ–ä¸æŒ‡å®šåƒæ•¸é€²å…¥äº’å‹•æ¨¡å¼")
    else:
        print("ğŸš€ å•Ÿå‹•äº’å‹•æ¨¡å¼...")
        asyncio.run(example_usage())
    
    end = time.time()
    t = end - start
    print(f"\nTotal run time: {t:.2f}s")